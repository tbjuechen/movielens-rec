import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import pandas as pd
import numpy as np
import pickle
from pathlib import Path
from loguru import logger
from tqdm import tqdm
from src.features.ranking_feature_engine import RankingFeatureEngine
from src.data_loader_listwise import ShardedListwiseRankingDataset
from src.models.ranking.deep_model import UnifiedDeepRanker

def dcg_at_k(r, k):
    r = np.asfarray(r)[:k]
    if r.size:
        return np.sum(r / np.log2(np.arange(2, r.size + 2)))
    return 0.

def ndcg_at_k(r, k):
    idcg = dcg_at_k(sorted(r, reverse=True), k)
    if not idcg: return 0.
    return dcg_at_k(r, k) / idcg

def evaluate_ranker():
    device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
    model_dir = Path("saved_models/unified_ranker")
    data_dir = Path("data/processed")
    ranking_dir = data_dir / "ranking"

    # 1. åŠ è½½å…ƒæ•°æ®ä¸Žæ¨¡åž‹
    logger.info(f"æ­£åœ¨ä»Ž {model_dir} åŠ è½½æ¨¡åž‹...")
    with open(model_dir / "model_meta.pkl", "rb") as f:
        meta = pickle.load(f)
    
    model = UnifiedDeepRanker(
        meta['feature_map'], 
        embedding_dim=meta['embedding_dim']
    ).to(device)
    model.load_state_dict(torch.load(model_dir / "model.pth", map_location=device))
    model.eval()

    # 2. å‡†å¤‡éªŒè¯é›† (ä¸¥æ ¼éš”ç¦»æœ€åŽ 20%)
    samples_df = pd.read_parquet(ranking_dir / "ranking_samples_prototype.parquet")
    samples_df = samples_df.sort_values('timestamp').reset_index(drop=True)
    
    n = len(samples_df)
    history_ratings = samples_df.iloc[:int(n*0.6)]
    history_ratings = history_ratings[history_ratings['label'] == 1][['userId', 'movieId', 'rating', 'timestamp']]
    
    val_samples = samples_df.iloc[int(n*0.8):].copy()
    
    # ðŸš€ é‡‡æ ·ä¼˜åŒ–ï¼šä¸ºäº†è¯„ä¼°æ•ˆçŽ‡ï¼Œéšæœºé‡‡æ · 50,000 æ¡è¿›è¡Œå¿«é€Ÿä½“æ£€
    if len(val_samples) > 50000:
        logger.info("éªŒè¯é›†è¿‡å¤§ï¼Œé‡‡æ · 50,000 æ¡è¿›è¡Œå¿«é€Ÿè¯„ä¼°...")
        val_samples = val_samples.sample(50000, random_state=42)
    
    engine = RankingFeatureEngine(data_dir=str(data_dir))
    engine.initialize(history_ratings)
    
    logger.info("æå–éªŒè¯é›†ç‰¹å¾çŸ©é˜µ...")
    val_df = engine.build_feature_matrix(val_samples)
    item_profile = pd.read_parquet(ranking_dir / "item_profile_ranking.parquet")
    
    # 3. æ•°æ®åŠ è½½å™¨ (Sharded Listwise)
    # å¯¹äºŽéªŒè¯é›†ï¼Œæˆ‘ä»¬å°† val_df å­˜ä¸ºä¸€ä¸ªä¸´æ—¶åˆ†ç‰‡è¿›è¡ŒåŠ è½½ï¼Œæˆ–è€…ç›´æŽ¥é€šè¿‡ mock è·¯å¾„åˆ—è¡¨
    # è¿™é‡Œä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬å…ˆå°† val_df å­˜ç›˜
    val_shard_path = ranking_dir / "val_shard_temp.parquet"
    val_df.to_parquet(val_shard_path, index=False)
    
    val_ds = ShardedListwiseRankingDataset([val_shard_path], item_profile, neg_ratio=4)
    # æ‰‹åŠ¨è§¦å‘åŠ è½½å½“å‰åˆ†ç‰‡
    val_ds._load_shard(0)
    
    # è¦†ç›–å…ƒæ•°æ®
    val_ds.mid_map = meta['mid_map']
    val_loader = DataLoader(val_ds, batch_size=512, shuffle=False)

    # 4. æŒ‡æ ‡è®¡ç®—
    metrics = {'top1_acc': [], 'mrr': [], 'ndcg5': []}
    
    logger.info("å¼€å§‹æ€§èƒ½è¯„ä¼°...")
    with torch.no_grad():
        for batch in tqdm(val_loader, desc="Evaluating"):
            B, K = batch['movieId'].shape
            inputs = {k: v.to(device) for k, v in batch.items() if k not in ['click_label', 'rating_label']}
            
            outputs = model(inputs)
            logits = outputs['click'].view(B, K) # (Batch, 5)
            
            for i in range(B):
                scores = logits[i].cpu().numpy()
                # çœŸå®žçš„æ­£æ ·æœ¬åœ¨ç´¢å¼• 0
                # å…¬å¹³æŽ’åè®¡ç®—ï¼šå¤„ç†å¹³åˆ†
                num_strictly_greater = (scores > scores[0]).sum()
                num_equal = (scores == scores[0]).sum()
                rank = num_strictly_greater + (num_equal + 1) / 2.0
                
                metrics['top1_acc'].append(1 if rank <= 1.5 else 0)
                metrics['mrr'].append(1.0 / rank)
                
                # NDCG è®¡ç®—
                pred_order = np.argsort(-scores)
                binary_rel = [1 if idx == 0 else 0 for idx in pred_order]
                metrics['ndcg5'].append(ndcg_at_k(binary_rel, 5))

    # 5. è¾“å‡ºæŠ¥å‘Š
    logger.success("æ€§èƒ½è¯„ä¼°å®Œæˆã€‚")
    print("\n" + "="*40)
    print("ç²¾æŽ’æ¨¡åž‹æ€§èƒ½æŠ¥å‘Š (Validation Set)")
    print("-" * 40)
    print(f"Top-1 Accuracy: {np.mean(metrics['top1_acc']):.4f}")
    print(f"Mean RR (MRR):  {np.mean(metrics['mrr']):.4f}")
    print(f"NDCG@5:         {np.mean(metrics['ndcg5']):.4f}")
    print("="*40 + "\n")

if __name__ == "__main__":
    evaluate_ranker()
